\documentclass[compress]{beamer}
\usepackage{setspace}
\usepackage{multicol}
\mode<presentation>
{
  %\usetheme{Warsaw}
  %\usecolortheme{spruce}
  % or ...
	%\useoutertheme{infolines}
  %\setbeamercovered{transparent}
  
  \usetheme{CambridgeUS}
    \setbeamercolor{item projected}{bg=darkred}
    \setbeamertemplate{enumerate items}[default]
    \setbeamertemplate{navigation symbols}{}
    \setbeamercovered{invisible}
    \setbeamercolor{block title}{fg=darkred}
    \setbeamercolor{local structure}{fg=darkred}
  
  % or whatever (possibly just delete it)
}

\usepackage{verbatim} 
\usepackage{listings}
\usepackage{tikz}

%\usepackage[noend]{algpseudocode}
\usepackage{algorithm,algorithmic}


\usepackage{amsmath}
\usepackage{blindtext}
\usepackage{wrapfig}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage[colorinlistoftodos]{todonotes}


\usetikzlibrary{arrows}
\usetikzlibrary{shapes}
\tikzstyle{block}=[draw opacity=0.7,line width=1.4cm]

\newcommand{\bigpause}{\bigskip \pause}
\newcommand\azul[1]{{\color{blue}#1}}
\lstloadlanguages{C++}
\lstnewenvironment{code}
	{%\lstset{	numbers=none, frame=lines, basicstyle=\small\ttfamily, }%
	 \csname lst@SetFirstLabel\endcsname}
	{\csname lst@SaveFirstLabel\endcsname}
\lstset{% general command to set parameter(s)
	language=C++, basicstyle=\footnotesize\sffamily, keywordstyle=\slshape,
	emph=[1]{tipo,usa}, emphstyle={[1]\sffamily\bfseries},
	basewidth={0.47em,0.40em},
	columns=fixed, fontadjust, resetmargins, xrightmargin=5pt, xleftmargin=15pt,
	flexiblecolumns=false, tabsize=2, breaklines,	breakatwhitespace=false, extendedchars=true,
	numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=9pt,
	frame=l, framesep=3pt,
}

\usepackage[spanish]{babel}
% or whatever

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\avg}{avg}
\DeclareMathOperator*{\stdev}{stdev}
\newcommand\normtwo[1]{\left\lVert#1\right\rVert_2}


\title[Defensa de tesis de licenciatura] % (optional, use only with long paper titles)
{Análisis y predicción de la búsqueda visual humana}

\author[Melanie Sclar] % (optional, use only with lots of authors)
{~Melanie Sclar}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.
\institute[UBA] % (optional, but mostly needed)
{
  %\inst{1}%
  Facultad de Ciencias Exactas y Naturales\\
  Universidad de Buenos Aires
}
%\date[PAP] % (optional, should be abbreviation of conference name)
%{Problemas, Algoritmos y Programación}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Contenidos}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

%\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}
\newcommand{\state}[1]{\left|\,#1\,\right\rangle}
\newcommand{\costate}[1]{\left\langle\,#1\,\right|}
\newcommand{\trace}{\text{Tr}}
\newcommand{\su}{\uparrow}
\newcommand{\sd}{\downarrow}
\newcommand{\im}{\text{Im}}
\newcommand{\re}{\text{Re}}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}


\begin{document}
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\begin{frame}
  \titlepage
\end{frame}


\section{Introducción}
\begin{frame}{Motivación}
\begin{itemize}
\item Hace más de medio siglo que se sabe que los lugares a los que miramos no son aleatorios, sin embargo todavía no existe una teoría formal de cómo funciona la visión humana.
\item Queremos entender cuáles son los patrones que describen las personas al buscar objetos en una imagen.
\item Más aún, queremos diseñar algoritmos que puedan simular la visión humana en una búsqueda visual.
%\item Hace más de una década que se viene fortaleciendo la aplicación de modelos bayesianos para realizar inferencias en entornos ruidosos y con mucha información, e hipotetizamos que la visión
\end{itemize}
\end{frame}

\begin{frame}{Agudeza visual y movimientos oculares}

  \begin{columns}[T]
    \begin{column}{.6\textwidth}
        \vspace{0.8cm}
        \begin{itemize}
        \item Solamente podemos ver en detalle solo una pequeña porción del campo visual.
        \item Debido a las limitaciones de agudeza en la retina, los movimientos oculares son necesarios para procesar detalles.
        \end{itemize}
    \end{column}
    \begin{column}{.4\textwidth}
        \begin{center}
        \includegraphics[width=\textwidth]{images/visibility-map-sin-ref.png} 
        \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/castelhano-fixations.jpg}
\end{center}

Las ubicaciones de las fijaciones no son aleatorias y los patrones que describen dependen fuertemente de la tarea que realice.
\end{frame}

\begin{frame}{Modelos de saliencia}

\begin{itemize}
\item Como no podemos procesar toda la información de una imagen a la vez, restringimos nuestra atención a un área reducida. 
\item ¿Cómo decidimos qué región seleccionar para captar nuestra atención primero?
\end{itemize}

\begin{center}
\includegraphics[width=0.9\textwidth]{images/ejemplo-mlnet.png}
\end{center}

% decir los diferentes factores de saliencia que existen
\end{frame}

\begin{frame}{Búsqueda visual}
\begin{itemize}
\item En una búsqueda visual a veces se ignoran regiones muy salientes porque son poco relevantes para la tarea.
\item La saliencia suele funcionar mejor en escenas artificiales que en naturales.
\item En contextos naturales hay que combinar saliencia con elementos que describan el target.
\end{itemize}
\end{frame}

\begin{frame}{Métricas de comparación de \textit{scanpaths}}
No hay consenso sobre qué métrica utilizar para comparar scanpaths, ya que hay muchos factores a tener en cuenta. Así, algunas de las más usadas son:
\begin{itemize}
\item Cantidad de fijaciones hasta encontrar el target
\item Ángulos entre fijaciones
\item String edit distance
\end{itemize}
\end{frame}

\begin{frame}{Métricas de comparación de \textit{scanpaths}}
{Edit distance - ejemplo}

\begin{center}
    \includegraphics[width=\linewidth]{images/edit-distance.png} 
\end{center}

\end{frame}


\section{Experimentos}
\subsection{Creación de un corpus de imágenes}
\begin{frame}{Creación de un corpus de imágenes de interiores}
\begin{itemize}
\item Recolectamos 134 imágenes de interiores de $768 \times 1024$ píxeles.
\item Las imágenes son en blanco y negro y no tienen personas ni texto.
\item Recortamos varios targets por imagen de diferentes y seleccionamos al azar uno por imagen, siempre y cuando fuera menor a $72 \times 72$.
\item Luego uniformizamos los targets para que todos tengan $72 \times 72$ píxeles.
\end{itemize}
\end{frame}

\subsection{Experimento de búsqueda visual}
\begin{frame}{Diseño del experimento de búsqueda visual}

\begin{itemize}
\item Utilizamos un Eye Tracker para grabar los movimientos oculares del ojo derecho.
\item Realizamos 134 ensayos, uno por imagen, en tres bloques. Entre cada bloque el participante descansa y se recalibra el Eye Tracker.
\item Al comienzo de cada ensayo se checkea que siga recalibrado.
\end{itemize}

\end{frame}

\begin{frame}
\begin{figure}[!b]
    \captionsetup{font=scriptsize,labelfont=scriptsize}
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/target_bathroom.jpg} 
        \caption{\tiny Muestra de target}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/first_fixation_bien.jpg} 
        \caption{\tiny Fijación forzada}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/full_bathroom.jpg} 
        \caption{\tiny Imagen completa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/subjective_response_ahora.jpg} 
        \caption{\tiny Respuesta subjetiva}
    \end{subfigure}
\end{figure}

\begin{itemize}
\item En (a) el target se muestra 3 segundos
\item En (b) los puntos están a 300 píxeles del target y se elige uno distinto por imagen.
\end{itemize}

\end{frame}

\begin{frame}
\begin{itemize}
\item Según el ensayo elegimos aleatoriamente la cantidad de sacadas que se le permite realizar a las personas antes de cortar el ensayo y pedirle el reporte subjetivo. 
\item Elegir una buena distribución de cantidad de fijaciones permitidas no fue fácil.%, pues quisimos maximizar la cantidad de ensayos con varias fijaciones donde los sujetos no hayan encontrado el target.
\end{itemize}

\bigskip

\begin{table}[h]
\centering
\tiny
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|}
\cline{2-10}
                                           & \multirow{2}{*}{\textbf{\tiny \begin{tabular}[c]{@{}c@{}} Cantidad \\ de sujetos\end{tabular}}} & \multirow{2}{*}{\textbf{\tiny \begin{tabular}[c]{@{}c@{}} Cantidad \\ de imágenes \end{tabular}}} & \multicolumn{7}{c|}{\textbf{Cantidad de sacadas}}                                        \\ \cline{4-10} 
                                           &                                                                                          &                                                                                                      & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{8} & \textbf{12} & \textbf{16} & \textbf{64} \\ \hline
\multicolumn{1}{|c|}{\textbf{Tarea 1}}     & 3                                                                                        & 108                                                                                                  & 20,4\%    &            & 20,4\%    & 20,4\%    &             & 20,4\%     & 18,4\%     \\ \hline
\multicolumn{1}{|c|}{\textbf{Tarea 2}}     & 2                                                                                        & 108                                                                                                  & 25\%       &            & 25\%       & 25\%       &             & 12,5\%     & 12,5\%     \\ \hline
\multicolumn{1}{|c|}{\textbf{Tarea 3}}     & 6                                                                                        & 108                                                                                                  & 25\%       & 25\%       & 25\%       & 25\%       &             &             &             \\ \hline
\multicolumn{1}{|c|}{\textbf{Tarea final}} & 17                                                                                       & 134                                                                                                  & 13,4\%    &            & 14,9\%    & 29,9\%    & 41,8\%     &             &             \\ \hline
\end{tabular}
\caption{\label{tab:tareas} Distribuciones de cantidad de sacadas utilizadas en la puesta a punto del experimento.}
\end{table}


\end{frame}

\subsection{Experimento de detección de objetos}
\begin{frame}{Experimento de detección de objetos}
\begin{itemize}
\item Mostramos cada uno de los target durante 3 segundos.
\item Pedimos que digan qué objeto es el de la imagen de tres formas distintas.
\item Si el usuario no reconoce el objeto, existe una opción a ese fin.
%\item Esta tarea se puede realizar online en varios días. Si quieren participar, sigue abierta en \texttt{http://objetos.gpoesia.com}
\item Obtuvimos en promedio 10.5 respuestas por imagen.
\end{itemize}
\end{frame}

\subsection{Análisis del comportamiento}

\begin{frame}{Longitud de las sacadas}
\begin{center}
\includegraphics[width=0.6\textwidth]{images/mean-sacclen.png}
\end{center}

Vemos que la longitud de las sacadas decrece a lo largo del tiempo, consistentemente con otros trabajos.
\end{frame}

\begin{frame}{Sesgo de la fijación central}
\begin{center}
\includegraphics[width=0.9\textwidth]{images/heatmap-per-fix.png}
\end{center}

Notar que el sesgo también se marca en la primera fijación. \end{frame}

%\begin{frame}

%\begin{center}
%\includegraphics[width=0.6\textwidth]{images/scatter-first-fixation.png}
%\end{center}

%{\scriptsize
%Notar que el sesgo también se marca en la primera fijación. Algunos sujetos movieron rápidamente los ojos desde la fijación pedida hacia el centro, y grabamos la primera fijación allí.}

%\bigskip

%{\scriptsize Se observa que la distancia del punto de la primera fijación al centro de la imagen correlaciona con cuán bien los participantes respetan la instrucción de mantener la mirada en el punto pedido.}
%\end{frame}

\begin{frame}{Cambios en incerteza según cantidad de fijaciones}
\centering
  \begin{columns}[T]
    \begin{column}{.4\textwidth}
        \vspace{1.3cm}
        \begin{itemize}
        \item Los sujetos tienen consciencia de que encontraron el target.
        \item Vimos que existe un efecto ``desmoralizador'' entre ensayos.
        \end{itemize}
    \end{column}
    \begin{column}{.6\textwidth}
        \begin{center}
        \centering
        \includegraphics[width=\linewidth]{images/mean_circle_size.png}
        \end{center}
    \end{column}
  \end{columns}
\end{frame}

\section{Modelos predictivos}
\subsection{Modelos estáticos}

\begin{frame}
Explicaremos los distintos modelos predictivos que proponemos. Estos modelos se dividen en dos clases según si predicen las regiones a ser fijadas (\textbf{modelos estáticos}) o si predicen el orden de las fijaciones (\textbf{modelos dinámicos}).

\bigskip

Primero veremos los modelos estáticos y el mejor de ellos será utilizado como base para los modelos dinámicos.

\bigskip

Para entender los modelos estáticos es clave entender cómo medimos su performance.
\end{frame}

\begin{frame}{Medidas de performance}

\begin{itemize}
{\small 
\item Tomamos los mapas de saliencia como un clasificador binario de los píxeles. Un porcentaje dado de los píxeles de una imagen se clasifican como positivos y el resto negativos.
\item Se grafica para varios porcentajes el True Positive Rate (TPR), que indica qué porción de las fijaciones caen en la región saliente.
}
\end{itemize}

\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/grayscale_100_oliva.jpg}
        \caption{\footnotesize Imagen original} \label{fig:grayscale_100_oliva}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/example-salient-percent-8-14.png} 
        \caption{\footnotesize 8.14\% más saliente de la imagen según SAM} \label{fig:example-salient-percent-8-14}
    \end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/example-salient-percent-27-73.png} 
        \caption{\footnotesize
27.73\% más saliente de la imagen según SAM} \label{fig:example-salient-percent-27-73}
    \end{subfigure}
\end{figure}

\end{frame}

\begin{frame}{Modelos de saliencia como predictores de fijaciones}

\begin{figure}
\includegraphics[width=0.5\linewidth]{images/sam_all_fix_small.png} 
\caption{Performance de SAM}
\end{figure}

{\footnotesize Tanto SAM como MLNet predicen mejor las primeras fijaciones que las últimas. Veamos cómo es el modelo de Judd y como lo extenderemos.}

\end{frame}

\begin{frame}{Modelo de saliencia de Judd}
\begin{itemize}
\item Se realiza un mapa de saliencia por imagen con las fijaciones humanas. Este mapa es tomado como \textit{ground truth}.
\item Judd toma 10 píxeles al azar del 20\% más saliente del mapa de saliencia \textit{ground truth} y los identifica como píxeles salientes. Asimismo, toma 10 píxeles al azar del 30\% menos saliente y los toma como píxeles negativos.
\item Computa 33 features por imagen.
\item Separa las imágenes en dos conjuntos: uno de entrenamiento y uno de testing. 
\item Entrena una Support Vector Machine (SVM) con las muestras positivas y negativas del conjunto de imágenes de entrenamiento.
\item \textbf{Este modelo permite muy fácilmente agregar y remover features} que se adapten mejor a nuestra tarea.
\end{itemize}
\end{frame}

\begin{frame}{Modificaciones nuestras a Judd}
\begin{itemize}
\item Removemos las features que no son relevantes a nuestra tarea.
\bigskip
\item Extendemos Judd con dos tipos de mapas que extraen información del target:
\begin{enumerate}[I.]
\item Mapas teniendo en cuenta el aspecto netamente visual del target.
\item Mapas teniendo en cuenta la semántica de la imagen mostrada.
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Extensión de Judd - tipo I}

\begin{itemize}
\item El primer mapa es del de la \textit{cross correlation} del target y la imagen original.
\item El segundo mapa intenta representar una \textbf{similitud de bajo nivel} entre el target y una porción de la imagen.
\end{itemize}

\begin{figure}
\includegraphics[width=0.7\linewidth]{images/grilla-gorda.png} 
\end{figure}

\end{frame}

\begin{frame}{Extensión de Judd - tipo II}

  \begin{columns}[T]
    \begin{column}{.65\textwidth}
        \begin{itemize}
            \item Queremos crear un mapa teniendo en cuenta la semántica de la imagen mostrada.
            \item Sabemos a qué clase de objeto pertenece cada target por nuestro experimento online. 
            \item Encontramos un dataset llamado LabelMe que posee 8000 imágenes segmentadas y parcialmente anotadas.
        \end{itemize}
    \end{column}
    \begin{column}{.35\textwidth}
        \vspace{0.5cm}
        \begin{center}
        \includegraphics[width=\textwidth]{images/label-me-example.png} 
        \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Outline del algoritmo del mapa de tipo II}
\begin{itemize}
\item Detectamos a qué clase de objeto pertenece cada target con una palabra en español con nuestra tarea online.
\item Medimos la distancia de la palabra que representa a cada target respecto de algunas palabras que denotan posición en una escena de interiores (por ejemplo, mesa, piso, techo, ventana, pared
- las llamaremos \textbf{palabras posicionales}). 
\item Se toma la palabra posicional más cercana y ésta representará la posición probable del target: esto hipotetizamos que será un indicio de los sectores de la imagen asociados a la palabra posicional. 
\item El mapa de features será entonces el mapa de las regiones más fuertemente asociadas a la palabra posicional.
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{images/labelme-global-features-without-horizon.png} 
\end{center}
\end{figure}

{\small Creamos un mapa posicional por cada una de las 16 palabras posicionales elegidas, superponiendo las regiones anotadas con la palabra en LabelMe.}
\end{frame}

\subsubsection{Resultados de modelos estáticos}
\begin{frame}{Resultados tercera fijación}
\begin{center}
\includegraphics[width=0.75\textwidth]{images/results_fix_3_to_3_all.png} 
\end{center}

%{\footnotesize En este gráfico no se separan los sujetos en conjunto de entrenamiento y conjunto de test, solo las imágenes.}
\end{frame}

%\begin{frame}
%\begin{itemize}
%\item También hicimos experimentaciones separando los sujetos en un conjunto de entrenamiento y otro de test (25\% para el test set, 75\% para el training set), donde se ve que la performance es muy similar a la alcanzada por los humanos.
%\item Como esto nos deja con muy pocos datos para el test set (a lo sumo $7 \times 34$) los resultados no son concluyentes.
%\item Además, no pudimos concluir cuáles de los 3 features que agregamos son los que tienen más peso, pues depende del conjunto de entrenamiento elegido.
%\end{itemize}
%\end{frame}

\begin{frame}{Predicción del centro del círculo de respuesta}
\begin{itemize}
\item Utilizamos el mismo modelo de Judd que acabamos de explicar, pero agregando una feature más que modele la distancia al target.
\item Esta feature ayuda a modelar el conocimiento del target. De todas formas, testearemos en todos los ensayos y luego sólo en aquellos donde el target no fue encontrado.
\end{itemize}
\end{frame}

\begin{frame}{Predicción del centro del círculo de respuesta}
{Predicción sobre todos los ensayos, comparando con los humanos. Separamos los sujetos en conjunto de training y test ($\frac{3}{4}$ y $\frac{1}{4}$)}

\begin{center}
\includegraphics[width=0.7\textwidth]{images/guess_all_test_2.png} 
\end{center}
\end{frame}

%\begin{frame}{Predicción del centro del círculo de respuesta}
%\begin{itemize}
%\item Si bien pudimos alcanzar performances muy similares a las humanas, deberíamos obtener datos de más sujetos para poder concluirlo fehacientemente.
%\item Por ser un modelo de machine learning, si bien logramos predicciones muy buenas todavía no podemos describir intuitivamente cuáles son las regiones más probables.
%\end{itemize}
%\end{frame}

\begin{frame}{Conclusión de los modelos estáticos}
\begin{itemize}
\item Superamos a los modelos del estado del arte en la predicción de la tercera fijación.
\item Predijimos eficazmente las ubicaciones de los centros de los círculos de respuesta.
\begin{itemize}
\item Por ser un modelo de machine learning, no podemos describir intuitivamente cuáles son las regiones más probables.
\item Sería bueno tener datos de más sujetos.
\end{itemize}
\end{itemize}

\bigskip

El modelo estático de la tercera fijación será utilizado como base para los modelos dinámicos.
\end{frame}

\subsection{Modelos dinámicos}
\begin{frame}{Modelos dinámicos}
Los modelos dinámicos son aquellos que no solo predicen las áreas donde
recaerán las fijaciones, sino que buscarán predecir posiciones concretas de las fijaciones y un orden probable de las mismas.

\bigskip

Para esto nos basaremos en un modelo bayesiano propuesto por Najemnik \& Geisler en 2005. 
\end{frame}

\begin{frame}{Modelo de Najemnik \& Geisler}

$$s_{i}(T) = prior(i) \cdot \prod_{t=1}^T exp\left(d'^2_{ik(t)} W_{ik(t)}\right)$$

\begin{itemize}
\item $T$ es la fijación actual
\item $k(t)$ es ubicación de la fijación número $t$
\item $prior$ es la probabilidad a priori
\item $d'_{ik(t)}$ y $W_{ik(t)}$ son el mapa de visibilidad y la respuesta del target en la ubicación $i$ cuando el sujeto se encuentra fijando la vista en $k(t)$
\item Nosotros aproximamos $d'_{ik(t)}$ pues otros modelos más sofisticados eran prohibitivos para nuestra tarea
\end{itemize}
\end{frame}

%\begin{frame}

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.5\textwidth]{images/visibility-map.png} 
%\end{center}
%\end{figure}

%Ejemplo de mapa de visibilidad, que es lo que modela $d'$. En nuestro trabajo lo simplificamos como una gaussiana bidimensional con los parámetros de forma que imiten a los de la imagen (extraída del paper de Najemnik \& Geisler).%, pero existen otros modelos más sofisticados y computacionalmente prohibitivos para el presente trabajo, que queda como trabajo futuro. 

%\end{frame}

\begin{frame}

$$k_{opt}(T+1) = \argmax\limits_{k(T+1)} \sum_{i=1}^n p_i(T)p(C|i,k(T+1))$$

donde $p(C|i,k(T+1))$ es la probabilidad de estar en lo correcto dado que la ubicación del target es $i$ y la ubicación de la próxima fijación es $k(T+1)$.
\end{frame}

\begin{frame}{Definición de $W$}
\begin{itemize}
\item Dijimos que $W_{ik(t)}$ es la respuesta del template de la $i$-ésima posible ubicación dado que el sujeto está fijando en $k(t)$ en la fijación número $t$. 

\item En otras palabras, representa cuán parecida es la posición $i$ al target para el observador que se encuentra fijando su vista en $k(t)$. 
\item Definimos $W_{ik(t)} \in \mathcal{N}(\mu_{ik(t)}, \sigma^2_{ik(t)})$ donde:

$$ \mu_{ik(t)}
= \left\{ \begin{array}{lc}
             0.5 &  si \ i = \ ubicaci\acute{o}n \ del \ target \\
             -0.5 &  caso \ contrario 
          \end{array}
   \right.$$

$$ \sigma^2_{ik(t)} = \displaystyle\frac{1}{d'^2_{ik(t)}}$$ 
\end{itemize}
\end{frame}

\begin{frame}{Propiedades del modelo de Najemnik \& Geisler}
\begin{itemize}
\item Este modelo logra simular inhibición de retorno y longitudes de sacadas moderadas implícitamente. 
\item Por su naturaleza probabilística es capaz de predecir varios scanpaths distintos según como se fije $W$.
\end{itemize}
\end{frame}

\begin{frame}{Aplicando el modelo de Geisler a nuestra tarea}
\begin{itemize}
\item El modelo de Najemnik \& Geisler fue realizado sobre una escena artificial, y por primera vez se lo aplica en escenas naturales. 
\item Tuvimos que discretizar nuestra imagen en una grilla de $15 \times 20$ para poder computar el algoritmo en un tiempo razonable.
\item Utilizamos como $prior$ el mapa de saliencia de Judd extendido con nuestras features.
\end{itemize}
\end{frame}

\begin{frame}{Aplicando el modelo de Geisler a nuestra tarea (II)}
\begin{itemize}
\item Como la visibilidad tiende a cero cuando un punto está muy lejos de donde se está realizando la fijación, la varianza tendería a infinito. Es por esto que trasladamos la varianza de la siguiente forma:

$$ \sigma_{ik(t)}^1 = \displaystyle\frac{1}{a \cdot d'_{ik(t)} + b} \text{ con } a = 3 \text{ y } b = 4$$ 

Además experimentamos con otros pares $(a,b)$.
\item Además de usar $W$ como fue definido originalmente, experimentamos con una modificación 

$$\mu_{ik(t)}^2 = \mu^1_{ik(t)} \cdot \Big(d'_{ik(t)} + \dfrac{1}{2}\Big) + corr_{i} \cdot \Big(\dfrac{3}{2} - d'_{ik(t)}\Big) \in [-1, 1]$$

\end{itemize}
\end{frame}

\begin{frame}{Otros modelos dinámicos}
\begin{itemize}
\item Además de los dos modelos basados en Geisler, agregamos otros tres modelos $baseline$ para comparar performance.
\begin{itemize}
\item Modelo de sesgo de la fijación central
\item Modelo dinámico estadístico
\item Modelo $greedy$
\end{itemize}
\end{itemize}

\bigskip
¿Cómo hacemos para comparar la performance de todos los modelos?

\end{frame}

\subsubsection{Métodos de comparación de scanpaths adaptados a nuestra tarea}
\begin{frame}{Métodos de comparación de scanpaths adaptados a nuestra tarea}

\begin{itemize}
\item Los métodos de comparación que mencionamos no siempre se adaptan bien a nuestra tarea, así que debimos adaptarlos.
\item Además, en los trabajos que vimos se suele hacer un análisis de consistencia entre sujetos pero no cuán parecido es un modelo predictivo a los humanos.
\end{itemize}
\end{frame}

\begin{frame}{Métricas utilizadas}
\begin{itemize}
\item Métrica de número de fijaciones esperadas para
encontrar el target, solo para ensayos exitosos.
\item Métrica de performance sobre todos los ensayos: calcula la proporción de ensayos exitosos promedio entre todos los ensayos con $c$ sacadas permitidas $c = \{2,4,8,12\}$.
\item Métrica de string edit distance.
\end{itemize}
\end{frame}

\begin{frame}{Métricas utilizadas}
{Métrica de número de fijaciones esperadas para
encontrar el target, solo para ensayos exitosos.}

Sea $s \in Sujetos$. Tomaremos todos los demás sujetos como conjunto de entrenamiento. Sea $$ E = \{(s,m) \ | \ \text{el sujeto $s$ encontró el target en la imagen $m$}\}$$

se tiene que

$$ \mu_m
= \left\{ \begin{array}{lc}
             \avg\limits_{j:(j,m) \in E} cf_{j,m} &  si \ |\{j:(j,m) \in E\}| \geq 3 \\
             +\infty &  \text{caso contrario}
          \end{array}
   \right.$$

$$mean\_length_s = \avg\limits_{m:(s,m) \in E} |cf_{s,m} - \mu_m| $$

$mean\_length_s$ es el valor que tomaremos como métrica.
\end{frame}

\begin{frame}{Métricas utilizadas}
{Métrica de string editing distance}

Sea $s \in Sujetos$. Calcularemos la edit distance media por imagen de todos los scanpaths de longitud al menos $c$, truncando solo las primeras $c$ fijaciones ($c \in \{2,4,6,8,10\}$). Sea $\mu_{m,c}^{lev}$ a la string edit distance media de la imagen $m$ en la categoría $c$.

$$\mu_{m,c,s}^{lev} = \avg\limits_{s \neq i} \ lev(scanpath_{s,m}[1..c], scanpath_{i,m}[1..c])$$

$$mean\_lev_{s,c} = \avg\limits_{1 \leq m \leq |images|} \ |\mu_{m,c}^{lev} - \mu_{m,c,s}^{lev}|$$

$mean\_lev_{s,c}$ es el valor que tomaremos como métrica.
\end{frame}

\begin{frame}
\begin{itemize}
\item Repetimos el procedimiento separando cada vez un sujeto $s$ diferente.
\item Luego, cuando queremos comparar un nuevo modelo contra los humanos, tomamos a $s$ como el nuevo modelo y todos los humanos irán al conjunto de entrenamiento.
\item ¿Cómo cuantificamos cuán parecido es un modelo a los sujetos?
\end{itemize}

\bigskip
\pause

Como los datos no están fuertemente sesgados, calculamos la media y la varianza y vemos cuán similar es un valor $x$ al resto de la muestra utilizando el \textit{z-score}.

\end{frame}

\subsubsection{Resultados de modelos dinámicos}
\begin{frame}{Resultados respecto de la primera métrica}
{Métrica de número de fijaciones esperadas para
encontrar el target, solo para ensayos exitosos $z_{length}^s$.}

\begin{center}
\includegraphics[width=0.75\textwidth]{images/z-score-mean-length.png}
\end{center}
{\scriptsize Recordemos que como utilizamos \textit{z-score}, \textbf{cuanto menor el valor obtenido, mejor.}}
\end{frame}

\begin{frame}{Resultados respecto de la segunda métrica}
{Métrica de performance sobre todos los ensayos: calcula la proporción de ensayos exitosos promedio entre todos los ensayos con $c$ sacadas permitidas $c = \{2,4,8,12\}$}

\begin{center}
\includegraphics[width=0.85\textwidth]{images/z-score-percent-found.png}
\end{center}

\end{frame}

\begin{frame}{Resultados respecto de la tercera métrica}
{Métrica de string edit distance}

\begin{center}
\includegraphics[width=0.9\textwidth]{images/z-score-mean-edit-distance.png}
\end{center}

\end{frame}

%\begin{frame}{Resultados respecto de la tercera métrica}
%{Métrica de string edit distance}
%\begin{itemize}
%\item La performance de los modelos es muy similar para aquellos con baja variabilidad, es decir, para pares $(a, b)$ más altos. \item La performance parece mejorar levemente para modelos de más alta varianza. Esto puede producirse porque al bajar la variabilidad el modelo simula un poco peor la existencia de distractores y equivocaciones en el juzgamiento de la mejor posición para obtener el target.
%\item \textbf{Se observa una mejora utilizando nuestro modelo de saliencia como base respecto de MLNet.}
%\end{itemize}
%\end{frame}

\begin{frame}{Conclusiones de los modelos dinámicos}
\begin{itemize}
\item El modelo $greedy$ obtuvo una performance mucho peor que los modelos bayesianos
\item Los modelos no bayesianos tuvieron performance muy pobres
\item El modelo de Geisler original con constantes $(3,2)$ o $(3,4)$ o el modelo de Geisler modificado con constantes $(3,4)$ parecen buenas elecciones
\item Se observa una mejora utilizando nuestro modelo de saliencia como base respecto de MLNet
\end{itemize}
\end{frame}

\section{Conclusiones}
\begin{frame}{Conclusiones generales}
\begin{itemize}
\item Creamos un corpus de imágenes y de descripción de objetos
\item Creamos un modelo de saliencia que predice mejor la tercera fijación que los modelos del estado del arte
\item Predijimos con los centros de los círculos de respuesta
\item Reprodujimos por primera vez el modelo de Geisler en escenas naturales
\end{itemize}
\end{frame}

\begin{frame}{Trabajos futuros}
\begin{itemize}
\item Amplificar el corpus de imágenes y de descripción de objetos
\item Explorar distintos aspectos de los features tipo II
\item Utilizar un mapa de visibilidad más sofisticado, por ejemplo el de Bradley et al. 2004.
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\Huge ¡Gracias!

\bigskip

\huge ¿Preguntas?
\end{center}
\end{frame}

\end{document}
